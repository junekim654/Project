---
title: "Predecting_Model"
author: "Taiga Hasegawa(taigah2)"
date: "2019/4/12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
train_X=read.csv("train_X.csv")[,-1]
train_y=read.csv("train_y.csv")[,-1]
test_X=read.csv("test_X.csv")[,-1]
test_y=read.csv("test_y.csv")[,-1]
train=cbind(train_X,train_y)
train_final=drop_na(train)
test=cbind(test_X,test_y)
test_final=drop_na(test)
```

#PCA
This data has too many numerical variables and they may cause colinearity. 
```{r}
#We must omit variables unnecessary for modeling
#Pts variable should also be omitted because this is the same with match result (i.e. Point difference between two team =result ) 
#We can use estimated Pts_diff only for test dataset but this time we also omitted this variable from testdata as conviniece
train_final_for_pca=train_final[,c(-2,-3,-4,-5,-6,-7,-46)]

test_final_for_pca=test_final[,c(-2,-3,-4,-5,-6,-7,-46)]

pca=prcomp(train_final_for_pca,scale. = TRUE)
plot(pca, type = "l", pch = 19)
```
```{r}
train_pca=pca$x[,1:9]
test_pca <- predict(pca, newdata =test_final_for_pca )
test_pca=data.frame(test_pca[,1:9])
```

#Logistic Regression
```{r}
library(glmnet)
```

```{r}
y=train_final[,46]
train_pca=data.frame(cbind(train_pca,y))
fit=glm(y~.,data = train_pca,family = binomial(link=logit))
summary(fit)
```
```{r}
predicted <- predict(fit, test_pca, type="response")

result.pred = rep(0, length(predicted))
result.pred[predicted > .5] = 1

table(result.pred,test_final[,46])
```
The result is quite good but we will try to use other model and Pts_diff variable.

#LDA
```{r}
library(MASS)
dig.lda=lda(train_pca[,1:9],y)
Ytest.pred=predict(dig.lda, test_pca[,1:9])$class
table(test_final[,46], Ytest.pred)

```

#QDA
```{r}
dig.qda=qda(train_pca[,1:9],y)
Ytest.pred=predict(dig.qda, test_pca[,1:9])$class
table(test_final[,46], Ytest.pred)
```

#Random Forest
```{r}
library(randomForest)
rf.fit = randomForest(train_pca[,1:9], as.factor(y), ntree = 1000, mtry = 6, nodesize = 10, sampsize = 500)
pred = predict(rf.fit, test_pca[,1:9])
table(test_final[,46], pred)
```

#Different criteria
In this section, we're also using Pts_diff (point difference between Team1 and Team2 estimated from previous game).
```{r}
result_from_pts=ifelse(test_final$Pts_diff>0,1,0)
table(result_from_pts,test_final[,46])
```
This is better than the result from logistic regression, LDA, Random Forest. It means it's deffinitely worth using "Pts_diff" variable.

